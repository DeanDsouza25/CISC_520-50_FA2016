'''
@author: Dean D'souza
'''
# Importing Required Libraries
import csv
import nltk
import re
import random
import math
from ProjectTokenizer import pToken
from ProjectTagger import pTagger
from nltk.corpus import stopwords
from nltk.text import Text

# Declaring empty holder for training set
allTweets=[]
# Reading in the data from the file
csvfile = open('C:/Users/demon/OneDrive/Documents/GitHub/CISC_520-50_FA2016/Project/data/FinalData.csv')
reader = csv.DictReader(csvfile, dialect='excel', fieldnames=['sentiment', 'ID', 'date', 'query', 'author', 'content'])
for row in reader:
    allTweets.append((row['content'], row['sentiment']))
csvfile.close()

# Separating positive and negative Tweets
negSet = [(e,s) for (e,s) in allTweets if s == '0']
posSet = [(e,s) for (e,s) in allTweets if s == '4']
# Defining separation parameter for each sentiment
negCutOff = int(math.floor(len(negSet)*0.8))
posCutOff = int(math.floor(len(posSet)*0.8))
# Creating Training and Testing sets
trainTweets = negSet[:negCutOff] + posSet[:posCutOff]
testTweets = negSet[negCutOff:] + posSet[posCutOff:]
# Randomizing the order of test and train Tweets
random.shuffle(trainTweets)
random.shuffle(testTweets)

# Function to create list of all words and their sentiment
def getWordsInTweets(tweets):
    wordsInTweets=[]
    for (words, sentiment) in tweets:
        filter1Words = ([w.lower() for w in pToken(words) if len(w)>2 and w not in stopwords.words('english')])
        filteredWords = pTagger(filter1Words) 
        for w in filteredWords:
            if (w[0][1] not in ['UserID','email', 'link']):
                wordsInTweets.append((w[0][0],sentiment))
    return wordsInTweets
# Function to get frequency distribution to be used as feature
def getWordFeatures(tWordList, num):
    # Creating empty frequency and conditional frequency distributions
    tfd = nltk.FreqDist()
    tcfd = nltk.ConditionalFreqDist()
    # Building the distribution
    for (word,sentiment) in tWordList:
        tfd.inc(word)
        tcfd[sentiment].inc(word)
    # Obtaining negative, positive and total word counts
    negCount = tcfd['0'].N()
    posCount = tcfd['4'].N()
    totalCount = negCount + posCount
    # Defining empty holder for word scores
    wScores={}
    # Calculating word scores
    for (word,freq) in tfd.iteritems():
        negScore = nltk.metrics.BigramAssocMeasures.chi_sq(tcfd['0'][word], (freq,negCount), totalCount)
        poScore = nltk.metrics.BigramAssocMeasures.chi_sq(tcfd['4'][word], (freq,posCount), totalCount)
        wScores[word] = negScore + poScore
    # Sorting based on word scores and then creating set of 'n' best word features 
    bestVals = sorted(wScores.iteritems(), key=lambda (w,s): s, reverse=True)[:num]
    bestFeats = set([w for (w,s) in bestVals])
    return bestFeats

# Using the getWordsInTweets() and getWordFeatures() to get the best features to use in the FeatExt()
tweetsFeats = getWordFeatures(getWordsInTweets(trainTweets),2000)
# Function to extract features from Tweets
def FeatExt(ttweet):
    tagdWords = pTagger([w.lower() for w in pToken(ttweet) if len(w)>2 and w not in stopwords.words('english')])
    tWords = []
    for w in tagdWords:
        if (w[0][1] not in ['UserID','email', 'link']):
            tWords.append(w[0][0])       
    feat = {}
    for e in tweetsFeats:
        feat['contains(%s)' % e] = (e in tWords)
    for e in tWords:
        if re.search('\!{2,}',e):
            feat['!']=True
    for e in tWords:
        if re.search('\?{2,}',e):
            feat['?']=True
    for e in tWords:
        if re.search('\.{3,}',e):
            feat['.']=True
    return feat
         
# Applying features to training set
trainTweetsApplied = nltk.classify.apply_features(FeatExt, trainTweets)

# Building the Classifiers
classifierNB = nltk.NaiveBayesClassifier.train(trainTweetsApplied)
        
# Most informative features
print(classifierNB.show_most_informative_features(30))

tsent = r'Thank uuuuuuuuu everyone!!!!!!! #justholdon'
tsent2 = tuple(tsent,'0')
tsentapplied = nltk.classify.apply_features(FeatExt,tsent2)
pred = classifierNB.classify(tsentapplied)
print("For the test sentence of %s = %s" % (tsent,pred))